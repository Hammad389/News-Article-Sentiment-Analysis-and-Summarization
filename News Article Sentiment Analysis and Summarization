{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537843cd-8acc-4166-a5b6-0e3f9e47f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from chromedriver_py import binary_path  # Path to the chromedriver executable\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Utility Functions\n",
    "# ==========================================\n",
    "\n",
    "def process_input(query: str) -> str:\n",
    "    \"\"\"Process the user's input to make it search-engine friendly.\"\"\"\n",
    "    return query.replace(' ', '+').upper()\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Sentiment Analysis Functions\n",
    "# ==========================================\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the sentiment analysis model.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "    \n",
    "    def analyze(self, text: str) -> dict:\n",
    "        \"\"\"Analyze sentiment of the given text.\"\"\"\n",
    "        try:\n",
    "            encoded_text = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            output = self.model(**encoded_text)\n",
    "            scores = softmax(output[0][0].detach().numpy())\n",
    "            sentiment = {\n",
    "                'Negative': scores[0],\n",
    "                'Neutral': scores[1],\n",
    "                'Positive': scores[2]\n",
    "            }\n",
    "            return sentiment\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article for sentiment analysis: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Summarization Functions\n",
    "# ==========================================\n",
    "\n",
    "class ArticleSummarizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the summarization pipeline.\"\"\"\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    def summarize(self, text: str) -> str:\n",
    "        \"\"\"Summarize the given text.\"\"\"\n",
    "        try:\n",
    "            summary = self.summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
    "            return summary[0]['summary_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while summarizing article: {e}\")\n",
    "            return \"Summary not available.\"\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Web Scraping Functions\n",
    "# ==========================================\n",
    "\n",
    "class ArticleScraper:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the scraper with default configurations.\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "        self.svc = ChromeService(executable_path=binary_path)\n",
    "        self.driver = webdriver.Chrome(service=self.svc, options=self.chrome_options)\n",
    "    \n",
    "    def scrape_links(self, search_entity: str) -> list:\n",
    "        \"\"\"Scrape article links based on a search query.\"\"\"\n",
    "        url = f'https://www.thenews.com.pk/search1?cx=014389848304649563256%3Aureyfztd74a&cof=FORID%3A10&ie=UTF-8&q={search_entity}'\n",
    "        self.driver.set_page_load_timeout(15)\n",
    "\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            page_source = self.driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        except TimeoutException:\n",
    "            self.driver.execute_script(\"window.stop()\")\n",
    "            page_source = self.driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        self.driver.quit()\n",
    "\n",
    "        div_tags = soup.find_all('div', {'class':'gsc-webResult gsc-result'})\n",
    "        href_list = [div.find('a').get('href') for div in div_tags if div.find('a')]\n",
    "        \n",
    "        return href_list\n",
    "\n",
    "    def scrape_article_body(self, url: str) -> dict:\n",
    "        \"\"\"Scrape the article body and extract the title and content.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            time.sleep(3)\n",
    "\n",
    "            title = soup.find('h1').get_text() if soup.find('h1') else 'No title found'\n",
    "            story_section = soup.find('div', {'class' : 'story-detail'})\n",
    "            paragraphs = story_section.find_all('p')\n",
    "            body = ''.join((p.get_text() for p in paragraphs[:-1]))\n",
    "\n",
    "            return {\"title\": title, \"body\": body, \"url\": url}\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Data Visualization Functions\n",
    "# ==========================================\n",
    "\n",
    "def visualize_data(data_frame: pd.DataFrame, search_entity: str):\n",
    "    \"\"\"Visualize the sentiment analysis results in a bar plot.\"\"\"\n",
    "    fig = plt.figure(figsize=(10, len(data_frame) * 3))\n",
    "    gs = GridSpec(len(data_frame), 2, width_ratios=[1, 2], height_ratios=[1]*len(data_frame), hspace=0.6)\n",
    "    \n",
    "    for index, article in data_frame.iterrows():\n",
    "        ax1 = fig.add_subplot(gs[index, 0])\n",
    "        sentiments = article['sentiment_score']\n",
    "        ax1.bar(sentiments.keys(), sentiments.values(), color=[\"#6CA0DC\", \"#FF6F61\", \"#FFD662\"])\n",
    "    \n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.set_ylabel(\"Sentiment Score\")\n",
    "        ax1.set_title(\"Sentiment Analysis\", fontsize=10)\n",
    "        \n",
    "        ax2 = fig.add_subplot(gs[index, 1])\n",
    "        ax2.axis(\"off\")\n",
    "        \n",
    "        summary_text = f\"**Summary**: {article['summary']}\\n\"\n",
    "        ax2.text(0, 0.8, summary_text, ha=\"left\", va=\"top\", wrap=True, fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f'News Articles Sentiment Analysis about \"{search_entity.capitalize()}\"', fontsize=14)\n",
    "    plt.subplots_adjust(top=0.96)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Main Execution Function\n",
    "# ==========================================\n",
    "\n",
    "def main():\n",
    "    input_string = input(\"Enter the name of the entity you want to search: \")\n",
    "    search_entity = process_input(input_string)\n",
    "\n",
    "    scraper = ArticleScraper()\n",
    "    links = scraper.scrape_links(search_entity)\n",
    "\n",
    "    sentiment_analyzer = SentimentAnalyzer()\n",
    "    summarizer = ArticleSummarizer()\n",
    "\n",
    "    articles_data = []\n",
    "    \n",
    "    for link in links:\n",
    "        article_details = scraper.scrape_article_body(link)\n",
    "        if article_details:\n",
    "            title = article_details['title']\n",
    "            body = article_details['body']\n",
    "            summary = summarizer.summarize(body)\n",
    "            sentiment_score = sentiment_analyzer.analyze(body)\n",
    "\n",
    "            if sentiment_score:\n",
    "                articles_data.append({\n",
    "                    'title': title,\n",
    "                    'summary': summary,\n",
    "                    'sentiment_score': sentiment_score\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(articles_data)\n",
    "    visualize_data(df, search_entity)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
